{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Segmentation and Modeling for CryoET - putting the Cell in Cellular Structural Biology","text":"<p>Segmentation and geometrical modeling for Cryo-ET are powerful processing steps that can be used to  generate better visualizations that capture the 3-dimensional nature of tomograms quantify ultrastructure of cellular features,  and provide supporting geometry for particle localization and subtomogram averaging.</p> <p>Because every cellular tomography project involves a different set of features, studying membranes, filaments, large protein complexes, or phase separation, the tools and techniques used for segmentation and modeling can vary widely. Today, we will cover several different software tools that are used for studying membranes and filaments in cryo-ET. If you are interested in studying these or other aspects of cellular ultrastructure, please reach out to me and I will be happy to help you find the right tools for your specific project, or to collaborate to develop new tools if needed - I am always looking for new projects!</p> <p>The three major tools we will cover today are:</p> <ul> <li>Membrain-Seg - a deep learning-based tool for segmenting membranes in cryo-ET data. This tool is nearly bulletproof for segmenting membranes, and does not require retraining. It is developed by Lorenz Lamm as part a collaboration between Ben Engel and Tingying Ping's research groups, and is part of the teamtomo community software development effort led loosely by Alister Burt.</li> <li>Dragonfly - a commercial software package that is widely used in the cryo-ET community for segmentation and modeling. Matt Swulius has developed a number of workflows for Dragonfly that are used for segmentation and modeling of cellular features. Dragonfly is especially nice for its user-friendly interface for generating training data and training and evaluating models for segmentation. This makes it great for studying your filament of choice, since it is flexible and does not rely on pretrained models.</li> <li>Surface Morphometrics - an open-source toolkit for building high quality triangle mesh models in a fully automated way from segmentations in Cryo-ET, and using those segmentations to quantify local and global membrane ultrastructure. This software was developed by me during my postdoc in Danielle Grotjahn's lab at Scripps Research, and my lab is continuing to develop new methodology within the framework. </li> </ul> <p>We'll also use the following tools for visualization and analysis:</p> <ul> <li>IMOD - a suite of tools for 3D reconstruction and modeling of tomographic data. We will use IMOD for visualization of tomograms and segmentations</li> <li>Meshlab - a powerful open-source tool for processing and editing 3D meshes. We will use Meshlab for manual generation of triangle meshes.</li> <li>Paraview - a powerful open-source tool for visualization and analysis of large datasets. We will use Paraview for visualization of quantifications on meshes.</li> </ul>"},{"location":"dragonfly/","title":"Dragonfly Segmentation","text":"<p>Dragonfly is a free-for-nonprofit software package that has a nice set of tools for generating training data, training a variety of different neural networks types, and applying them flexibly to different kinds of data. Jess Heebner in Matt Swulius's lab developed some excellent protocols for using matlab to segment tomography data, and showed that in high-contrast examples it is possible to differentiate actin from cofilactin, for example. They also established a simplified protocol for extracting filaments in amira from segmentations, massively reducing the human time required for that kind of analysis. Their jove article detailing this protocol can be found here: https://www.jove.com/t/64435/deep-learning-based-segmentation-of-cryo-electron-tomograms. What we are going to do today is a slight update from their original protocols, but ultimately you can't go wrong following their jove article. The only exception would be that I would advise not using the segmentation wizard, and instead using the deep learning toolbox. That is what we will do today!</p>"},{"location":"dragonfly/#launch-dragonfly","title":"Launch Dragonfly","text":"<p>Hopefully this is already done if you licensed everything and it went well!</p> <pre><code>module load dragonfly\nDragonfly\n</code></pre> <p>We will pause here for everyone to get set up with Dragonfly. If you have any issues, please let one of us know! Sometimes Dragonfly stalls when you launch it - if this happens, open a new tab and try launching dragonfly again. That should unstall the first one and you can kill the second job once it does (with Ctrl-C). Yes, I also hear how crazy that sounds as a strategy.</p>"},{"location":"dragonfly/#setting-up-dragonfly","title":"Setting up Dragonfly","text":"<p>Hopefully you all got Dragonfly licensed and opened earlier today. If you haven't, please do so now. If you have any issues, please let me know. </p> <p>Within dragonfly, we want to make one settings change to make life easier:</p> <ol> <li>Go to File -&gt; Preferences</li> <li>Change \"Default Unit\" to \"nanometers\" - most people do segmentations in nm scale, because that is what the volume EM community uses. The features we care about are usually 5+ nm in scale so this is a good default. All the software we are using will work fine in Angstrom scale, however.</li> </ol> <p>The software is a bit liable to crashing - to account for this, I'd recommend setting up autosave in File -&gt; Preferences -&gt; Autosave. I'd recommend saving every 5 minutes, and keeping 3 backups. This will save you a lot of headache if the software crashes.</p> <p>I anticipate this will take a bit of time to get everyone set up for everyone (licensing in particular) so if you get through quickly, feel free to grab a coffee!</p>"},{"location":"dragonfly/#loading-the-denoising-model","title":"Loading the Denoising Model","text":"<p>The first thing we are going to do is load a denoising model. This is a neural network that has been trained to remove noise from tomography data. This is not at all a critical step in the process, but it can make the hand segmentation process a little easier. We made this denoising model using Matt Swulius's CryoTomoSim software to generate noise free and noisy simulated data with lots of membranes, filaments, and other bits and bobs. It is not quite as accurate as cryoCARE or isonet, but it is fast and easy.</p> <p>To load the denoising model, go to the \"Deep Learning\" tab, and click \"Load Model\". Navigate to <code>/scratch/segmentation_dataset</code> and select the whole <code>tutorial</code> folder. This will load the denoising model, but you won't actually run it through the denoising tool (you can, but we won't today).</p>"},{"location":"dragonfly/#loading-the-data-and-the-membrain-segmentation","title":"Loading the Data and the Membrain Segmentation","text":"<p>Next, we are going to load the data. File -&gt; Open -&gt; navigate to <code>/scratch/segmentation_dataset</code> and select the <code>TE3_tomo.mrc</code> file. This is a tomogram showing a mitochondrion with some filaments in it. You should have gotten membrane segmentations of this data from the previous tutorial. If you haven't, you can find a usable segmentation in <code>/scratch/segmentation_dataset/morpho_run/datadir</code>. Load that file with the same File -&gt; Open process.</p>"},{"location":"dragonfly/#calibrating-the-data","title":"Calibrating the Data","text":"<p>Calibration puts all the data on a similar intensity scale. </p> <ol> <li>Check on the mode and standard deviation of the intensities in the data. On the data panel on the right side of the screen, with the tomogram selected, click on the histogram button under tools.</li> <li>Note down the mode and standard deviation of the data. Calculate the mode - standard deviation.</li> <li>Right click on the dataset and select Calibrate Intensity Scale. Set the foreground label to the mode and the background label to the mode - standard deviation. This will put the data on a consistent intensity scale.</li> </ol>"},{"location":"dragonfly/#preprocessing-the-image","title":"Preprocessing the Image","text":"<ol> <li>Workflows -&gt; Image Filtering</li> <li>Select \"Histogram Equalization\". Deselect the output.</li> <li>Click add and then select \"Gaussian\". Deselect the output.</li> <li>Click add and then select \"Unsharp\". Leave the output.</li> <li>Click \"Apply\" to apply the filter.</li> </ol> <p>This is where you can try out the denoiser in addition to the other \"standard\" processing steps. Instead of or in addition to doing the above steps, you can filter with AI:</p> <ol> <li>Main -&gt; Filter with AI</li> <li>Select the denoising model you loaded earlier.</li> <li>Click \"Filter\" to apply the filter.</li> </ol>"},{"location":"dragonfly/#preparing-to-train-the-neural-network","title":"Preparing to train the Neural Network","text":"<ol> <li>Convert the membrane segmentation to a multi-ROI by right-clicking on the segmentation and selecting \"Extract ROIs\". </li> <li>Delete the background ROI by clicking and trashing.</li> <li>Select the other ROI and right-click to \"Create Multi-ROI from ROI\". This will create a multi-ROI with the membrane pre-segmented.</li> <li>Add the following classes to the multi-ROI: \"Actin\", \"Septin\", \"Microtubule\", \"Ribosome\", \"Background\"</li> <li>Right click on the objects panel and select \"Create a box from\" and select current box. Choose to make the box 400 by 400 by 32 pixels.</li> <li>Move the box to a place where you can see some of each class of object. The 4-panel view can help with this.</li> <li>Switch to the segment panel and select the ROI painter -&gt; round brush, and select the OTSU brush. This will allow you to paint in the different classes of objects quickly and efficiently (if your calibration is good!)</li> <li>Start painting! Ctrl-left click to paint in the different classes of objects, and shift-left click to erase.</li> <li>Once you are done, </li> <li>Once you are done, right click on the box and select \"Add to ROI\", \"New ROI\", name it \"Mask\".</li> </ol>"},{"location":"dragonfly/#training-a-new-neural-network","title":"Training a new neural network","text":"<ol> <li>AI -&gt; Deep Learning Tool</li> <li>New Model</li> <li>\"U-Net\", \"Semantic Segmentation\", 6 classes (or 5 if you skipped ribosomes), 2.5D, 3 or 5 layers. More layers helps a lot but takes longer to train. </li> <li>Set up deep learning with that tool.</li> <li>Select the input data as input, the multi-ROI as output, and the mask as mask.</li> <li>Adjust the augmentation to have 2x augmentation. More is better, but we want to go fast not good today.</li> <li>Select visual feedback and select the input data.</li> <li>Adjust the training parameters:<ul> <li>Lower the epoch number. 25 is a good number that will go quick but not be low-quality.</li> <li>Increase batch size until the GPU memory is ~60% full.</li> </ul> </li> <li>Click \"Train\"</li> </ol> <p>Coffee break!!!</p>"},{"location":"dragonfly/#evaluating-the-neural-net-and-retraining","title":"Evaluating the neural net and retraining","text":"<p>Once the neural network is trained, you can apply it to the data and see how well it does. If it doesn't do well, you can continue to retrain it using the OTSU brush. At OHSU, we generally do 1 round of training with a pretrained network, then spend a bit of time repainting, then do a second round of training. This generally allows us to get a good to great segmentation with under an hour of human time.</p> <ol> <li>Segment Panel -&gt; Segment with AI</li> <li>Select the neural network you just trained.</li> <li>Click \"Segment\" (or \"Preview\" if you only want to see one slice)</li> </ol> <p>Optional: If the segmentation is bad, you can retrain the network with the new data.</p> <ol> <li>Use the otsu brush to correct any mistakes. You can also use the \"Fill\" tool to move large areas to background if needed.</li> <li>Repeat the training process once the correction is done. You might want to use a new box/mask to cover a relatively small number of larger slices - maybe 1k by 1k by 32 so you just clean up part of the data then train.</li> </ol>"},{"location":"dragonfly/#extracting-trained-data","title":"Extracting Trained Data","text":"<p>Once you have an inferred segmentation, you can extract the data as tifs then convert them to mrc. This is a bit of a pain, and ORS has a beta version of direct MRC export, so expect this to be much better in the future!</p> <ol> <li>Right click on the segmentation and select \"Convert to Greyscale\"</li> <li>Right click on the greyscale and select \"Export\" and select \"TIF\". Choose somewhere to save the data.</li> <li>Convert the stack of TIFs to MRC using IMOD. <pre><code>module load imod\ncd &lt;wherever you saved the tif files&gt;\ntif2mrc *.tif TE3_dragonfly_labels.mrc\n3dmod TE3_dragonfly_labels.mrc\n</code></pre></li> </ol>"},{"location":"membrain-seg/","title":"Membrain-Seg","text":"<p>Membrain-Seg is a relatively new tool for segmenting membranes in cryo-ET data. It is based on a deep learning model that has been trained on a variety of cellular tomograms. The model is able to segment membranes with high accuracy with a pretrained model, which makes it pretty much bulletproof as a first step for starting the segmentation process. If you want to do morphometrics, you can pretty quickly move from the Membrain-Seg output to a mesh using the <code>segmentation_to_meshes.py</code> script in the <code>surface_morphometrics</code> package (though I recommend you extract the individual components into their own classes first using dragonfly or amira).</p>"},{"location":"membrain-seg/#setup","title":"Setup","text":"<pre><code>conda activate membrain-seg\n</code></pre>"},{"location":"membrain-seg/#running-membrain-seg","title":"Running Membrain-Seg","text":"<pre><code>membrain segment --tomogram-path TE3_tomo.mrc --ckpt-path /sw/membrain-seg/models/MemBrain_seg_v10_alpha_MONAI1.3.0.ckpt --store-probabilities\n3dmod &lt;path_to_predictions&gt;\n</code></pre> <p>Usually, this is all you have to do. However, sometimes the default threshold is a bit too generous, and membranes merge into each other. This is bad. If this happens, you can try to adjust the threshold by running the following command:</p> <p><pre><code>3dmod &lt;path to scoremap&gt;\nmembrain thresholds --scoremap-path &lt;path_to_scoremap&gt;--thresholds X\n</code></pre> Determine X by looking at the pixel levels in the score map! You want to set the threshold so that the membranes are well separated.</p>"},{"location":"membrain-seg/#separating-components","title":"Separating components","text":"<p>I recommend using Amira or Dragonfly to separate components into semantic classes manually, but you can also use the following command to do it automatically:</p> <pre><code>membrain components --segmentation-path &lt;path-to-your-segmentation&gt; --connected-component-thres 50\n</code></pre> <p>And thats it! You should now have a set of membrane segmentations that are ready to be turned into meshes for morphometrics! For today's exercises, we will use these segmentations to help with dragonfly, but we will use some precalculated segmentations for morphometrics. If you are feeling brave, feel free to freelance during the morphometrics section and use this seg! It might be quite a bit better than the one we will be using.</p>"},{"location":"morphometrics/","title":"Surface Morphometrics","text":""},{"location":"morphometrics/#quantification-of-membrane-surfaces-segmented-from-cryo-et-or-other-volumetric-imaging","title":"Quantification of Membrane Surfaces Segmented from Cryo-ET or other volumetric imaging.","text":"<p>Surface morphometrics is a toolbox I developed during my postdoc to better understand how mitochondrial membranes remodel during stress. It turns out to be useful for a whole lot more than mitochondria! You can use it to assess aspects of membrane geometry in detail both locally and globally within cells, thanks to a robust meshing (modeling) algorithm and a set of simple tools for making measurements on the resulting surfaces.</p> <p>Today we will scratch the surface with a single tomogram, but one of the most valuable aspects of building models and making quantifications is that it becomes possible to assess statistical significance by comparing quantifications across many tomograms in different cells and conditions. This is a powerful way to move from qualitative phenomenology about cellular remodeling to robust quantitative understanding of the underlying biology. With one tomogram, we're mostly still doing phenomenology.</p>"},{"location":"morphometrics/#setup","title":"Setup:","text":"<ol> <li>Activate your conda environment and move to the morpho_run folder:  <pre><code>conda activate morphometrics\ncd /scratch/segmentation_dataset/morpho_run\n</code></pre></li> <li>Edit the <code>config.yml</code> file for our project's needs. I prefer visual studio code for this!<ul> <li>Set the <code>data_folder</code> to the folder containing your label file (<code>/scratch/segmentation_dataset/morpho_run/datadir</code>)</li> <li>Set the <code>output_folder</code> to the folder where you want the output files to be saved (<code>/scratch/segmentation_dataset/morpho_run/workdir</code>)</li> <li>For <code>segmentation_values</code>, use <code>-127</code> for the OMM and <code>-126</code> for the IMM.</li> <li>Set the <code>max_triangles</code> to 50,000 to speed up computation - this will reduce the quality of the final surfaces so don't do this when you are running at home!</li> <li>Set the <code>num_cores</code> to 16.</li> <li>Set the <code>radius_hit</code> to 10.</li> <li>Set <code>exclude_borders</code> to 1.</li> <li>For <code>intra</code>, provide <code>IMM</code> and <code>OMM</code></li> <li>For <code>inter</code>, provide <code>IMM</code>: <code>OMM</code></li> </ul> </li> </ol>"},{"location":"morphometrics/#example-data","title":"Example data","text":"<p>There is example data and a config available in the <code>morpho_run</code> folder. This is TE3_labels.mrc, which is the same tomogram you will be processing in the rest of the tutorial. You should check some details about it! You can also compare it to the tomogram itself. <pre><code>module load imod\nheader /scratch/segmentation_dataset/morpho_run/datadir/TE3_labels.mrc\nheader /scratch/segmentation_dataset/TE3_tomo.mrc\n3dmod /scratch/segmentation_dataset/morpho_run/datadir/TE3_labels.mrc\n3dmod /scratch/segmentation_dataset/TE3_tomo.mrc\n</code></pre></p>"},{"location":"morphometrics/#processing-your-data","title":"Processing your data","text":""},{"location":"morphometrics/#interactive-mesh-generation","title":"Interactive mesh generation","text":"<p>We are going to do semi-interactive mesh generation in meshlab to teach you how the sausage is made, but there is also a fully configurable pipeline (<code>python ../surface_morphometrics/segmentation_to_meshes.py config.yml</code>) that will do the whole thing for you.</p> <ol> <li>Prepare the xyz point cloud files to make new meshes:  <pre><code>cd /scratch/segmentation_dataset/morpho_run\npython ../surface_morphometrics/mrc2xyz.py -l -127 datadir/TE3_labels.mrc workdir/TE3_OMM.xyz\npython ../surface_morphometrics/mrc2xyz.py -l -126 datadir/TE3_labels.mrc workdir/TE3_IMM.xyz\n</code></pre></li> <li>Launch <code>Meshlab</code> <pre><code>module load meshlab\nmeshlab\n</code></pre></li> <li>For each mesh file:<ol> <li>Filters-&gt;Normals, Curvatures, and Orientation-&gt;Compute Normals for Point Sets</li> <li>Filters-&gt;Remeshing, Simplification, and Reconstruction-&gt;Screened Poisson Surface Reconstruction</li> <li>Filters-&gt;Selection-&gt;Select Faces by Vertex Quality</li> <li>Filters-&gt;Selection-&gt;Delete Selected Faces</li> <li>Filters-&gt;Remeshing, Simplification, and Reconstruction-&gt;Quadric Edge Collapse Decimation</li> <li>File-&gt;Export Mesh As...-&gt;TE3_OMM.ply</li> </ol> </li> </ol>"},{"location":"morphometrics/#pipelined-processing-steps","title":"Pipelined Processing Steps","text":"<ol> <li>If you wanted to make the meshes automatically: <code>python ../surface_morphometrics/segmentation_to_meshes.py config.yml</code>. We aren't doing this today because I think its more fun to do by hand. If you have 30 tomograms each with 3-5 labels, it will no longer be fun.</li> <li>Convert the ply files to vtp files: <code>python ../surface_morphometrics/ply2vtp.py config.yml TE3_OMM.ply TE3_OMM.surface.vtp</code>. Do this again for IMM.</li> <li>Run pycurv for each surface (normally, this is best run in parallel on a cluster):      <code>python ../surface_morphometrics/run_pycurv.py config.yml TE3_OMM.surface.vtp</code>. Do this again for IMM (it will take a long time for IMM!)     You may see warnings aobut the curvature, this is normal and you do not need to worry.     We may run into memory constraints - if you do, you can reduce the number of triangles in the surface by setting <code>max_triangles</code> to a lower number in the config file. This will reduce the quality of the final surfaces, but will make the computation faster and lower-memory.</li> <li>Measure intra- and inter-surface distances and orientations (also best to run this one in parallel for each original segmentation): <code>python ../surface_morphometrics/measure_distances_orientations.py config.yml</code></li> <li>Don't do this today: Combine the results of the pycurv analysis into aggregate Experiments and generate statistics and plots. This requires some manual coding using the Experiment class and its associated methods in the <code>morphometrics_stats.py</code>. Everything is roughly organized around working with the CSVs in pandas dataframes. Running  <code>morphometrics_stats.py</code> as a script with the config file and a filename will output a pickle file with an assembled \"experiment\" object for all the tomos in the data folder. Reusing a pickle file will make your life way easier if you have dozens of tomograms to work with, but it doesn't save too much time with just the example data...</li> </ol> <p>Just in case you have problems running things fast (I don't have a great sense of how long this will take on these machines), I have provided usable output files for the downstream analysis you might want to check out. You can find them in the <code>morpho_run/workdir</code> folder.</p>"},{"location":"morphometrics/#inspecting-results","title":"Inspecting Results","text":""},{"location":"morphometrics/#visualizing-the-surfaces","title":"Visualizing the surfaces","text":"<ol> <li>Load the surfaces in Paraview: <ul> <li>Open Paraview</li> <li>File -&gt; Open -&gt; TE3_OMM.surface.vtp</li> <li>File -&gt; Open -&gt; TE3_IMM.surface.vtp</li> <li>Make each visible</li> <li>Color by <code>curvedness_vv</code> to see the curvature of the surface</li> <li>Color by <code>OMM_dist</code> (or <code>IMM_dist</code>) to see the distance from the IMM to the OMM or visa-versa</li> </ul> </li> <li>Adjust color scales to see the features you are interested in. Add and edit the scalebar.</li> <li>Adjust the background, turn on ambient occlusion, and make the surfaces look nice for a screenshot.</li> <li>Take some nice pictures!</li> </ol>"},{"location":"morphometrics/#generating-some-basic-statistics-and-plots","title":"Generating some basic statistics and plots","text":"<ul> <li><code>python ../surface_morphometrics/single_file_histogram.py workdir/TE3_IMM.AVV_rh8.csv -n curvedness_vv</code> will generate an area-weighted histogram for a feature of interest in a single tomogram. I am using a variant of this script to respond to reviews asking for more per-tomogram visualizations!</li> <li><code>python ../surface_morphometrics/single_file_2d.py workdir/TE3_IMM.AVV_rh8.csv -n1 curvedness_vv -n2 OMM_dist</code> will generate a 2D histogram for 2 features of interest for a single surface.</li> </ul>"},{"location":"morphometrics/#running-individual-steps-without-pipelining","title":"Running individual steps without pipelining","text":"<p>Individual steps are available as click commands in the terminal, and as functions</p> <ol> <li>Robust Mesh Generation<ol> <li><code>mrc2xyz.py</code> to prepare point clouds from voxel segmentation</li> <li><code>xyz2ply.py</code> to perform screened poisson reconstruction and mask the surface</li> <li><code>ply2vtp.py</code> to convert ply files to vtp files ready for pycurv</li> </ol> </li> <li>Surface Morphology Extraction<ol> <li><code>curvature.py</code> to run pycurv in an organized way on pregenerated surfaces</li> <li><code>intradistance_verticality.py</code> to generate distance metrics and verticality measurements within a surface.</li> <li><code>interdistance_orientation.py</code> to generate distance metrics and orientation measurements between surfaces.</li> <li>Outputs: gt graphs for further analysis, vtp files for paraview visualization, and CSV files for         pandas-based plotting and statistics</li> </ol> </li> <li>Morphometric Quantification - there is no click function for this, as the questions answered depend on the biological system of interest!<ol> <li><code>morphometrics_stats.py</code> is a set of classes and functions to generate graphs and statistics with pandas.</li> <li>Paraview for 3D surface mapping of quantifications.</li> </ol> </li> </ol>"},{"location":"morphometrics/#summary-of-file-types","title":"Summary of File Types:","text":"<ul> <li>Files with.xyz extension are point clouds converted, in nm or angstrom scale. This is a flat text file with <code>X Y Z</code> coordinates in each line.</li> <li>Files with .ply extension are the surface meshes (in a binary format), which will be scaled in nm or angstrom scale, and work in many different softwares, including Meshlab. </li> <li>Files with surface.vtp extension are the same surface meshes in the VTK format.         * The .surface.vtp files are a less cross-compatible format, so you can't use them with as many types of software, but they are able to store all the fun quantifications you'll do!. Paraview or pyvista can load this format. This is the format pycurv reads to build graphs.</li> <li>Files with .gt extension are triangle graph files using the <code>graph-tool</code> python toolkit. These graphs enable rapid neighbor-wise operations such as tensor voting, but are not especially useful for manual inspection.</li> <li>Files with .csv extension are quantification outputs per-triangle. These are the files you'll use to generate statistics and plots.</li> <li>Files with .log extension are log files, mostly from the output of the pycurv run.</li> <li>Quantifications (plots and statistical tests) are output in csv, svg, and png formats. </li> </ul>"},{"location":"setup/","title":"Setup Steps","text":"<p>Grab your 3-button mice! Segmenting without one is the wooooooooorst!</p> <p>Your setup should be quite quick, as everything should already be arranged. However, we need to activate dragonfly. To do so, run the following commands (the last one is just to launch dragonfly and make sure it comes up successfully)</p> <pre><code>conda activate dragonfly\nbash ~/bin/dragonfly-register.bash\nDragonfly\n</code></pre> <p>Make sure you are able to open the Deep Learning Toolbox within dragonfly. You don't need it now but sometimes it complains about \"extra instances\". Raise your hand and get my attention and I will try to help if you run into this!</p>"},{"location":"setup/#finding-the-data","title":"Finding the data","text":"<p>I have provided some fun data for everyone to play with. Instead of working with beautiful perfect data, we will be playing with some \"typical\" data for a relatively new user. I collected it during the first year of my postdoc and I am quite fond of it so please don't hate too hard. The reality is that better data quality will in turn make segmentations better, but you should not be afraid to try to analyze imperfect data!</p> <p>The data can be found at: <pre><code>cd /scratch/segmentation_dataset\n</code></pre></p> <p>Take a look around in here. Feel free to open stuff with 3dmod and get a sense of what is in the tomogram (maybe easier to see in the deconv version). We are going to:</p> <ol> <li>Use membrain-seg to segment the membranes in the tomogram.</li> <li>Use dragonfly to do some quick contrast enhancement and train it to segment the 3 classes of filaments.</li> <li>Use surface_morphometrics to generate some meshes and do some basic quantifications on the data.</li> </ol>"}]}